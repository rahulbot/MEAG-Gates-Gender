{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MongoDB collection of quotes from a corpus of Media Cloud stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option to include *only* scare quotes and titles or *exclude* all scare quotes and titles. \"Scare quotes\" are defined as any quotes comprising three or fewer words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline uses the python implementation of MongoDB, [pymongo](https://pymongo.readthedocs.io/en/stable/). See end of notebook for handy pymongo commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pymongo\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This intializes MongoDB client on local server. For remote server, use format `'mongodb://SERVER_URL'`.  \n",
    "\n",
    "You need to create a database in that client and then a collection in that database. Your collection is where your stories and quotes will live. You can create and use multiple collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "database = client[\"media-cloud-client\"]\n",
    "\n",
    "collection = database.quotes_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Get Stories Text from Media Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use env variable for `MC_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIA_CLOUD_API_KEY = MC_API_KEY\n",
    "\n",
    "import mediacloud.api, json, datetime\n",
    "mc = mediacloud.api.AdminMediaCloud(MEDIA_CLOUD_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend fetching stories from Media Cloud in batches using `mc.storyList`. Reference API [documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2stories_publiclist) for list to build your query.  \n",
    "\n",
    "Since you can only fetch 1000 stories at a time through the Media Cloud API, those code splits the `SAMPLE_SIZE` of your corpus into batches and makes multiple calls. `CALL_SIZE` is the number of stories you'll fetch through the Media Cloud API. While the hard maximum for `CALL_SIZE` is 1000, a smaller `CALL_SIZE` tends to run faster.\n",
    "\n",
    "Use `sort=mc.SORT_RANDOM` to randomize stories fetched in query.  \n",
    "\n",
    "This code only adds stories with `stories_id` that do not already exist in your collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = SIZE_OF_CORPUS\n",
    "CALL_SIZE = BATCH_SIZE_FOR_FETCH_NO_GREATER_THAN_1000\n",
    "last_processed_stories_id = 0\n",
    "query = 'QUERY'\n",
    "i = collection.count_documents({})\n",
    "\n",
    "while i < SAMPLE_SIZE:\n",
    "    stories = mc.storyList(query,\n",
    "                        mc.publish_date_query(datetime.date(2012, 1, 1), datetime.date(2020, 4, 15)),\n",
    "                        #sort=mc.SORT_RANDOM,\n",
    "                        rows=CALL_SIZE,\n",
    "                        last_processed_stories_id=last_processed_stories_id,\n",
    "                        text=True)\n",
    "    last_processed_stories_id = stories[-1]['processed_stories_id']\n",
    "    for s in stories:\n",
    "        if collection.find_one({'stories_id': s['stories_id']}) is None and i < sample_size:\n",
    "            collection.insert_one(s)\n",
    "    i = collection.count_documents({})\n",
    "    \n",
    "print('{} stories added to collection'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Run Stanford CoreNLP\n",
    "\n",
    "This phase can take a while to run, so we built a small job-based app to do it. Check out the [Quote-Annotator](https://github.com/mitmedialab/Quote-Annotator) to see how to add quotes to all the stories fetched in the previous phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of stories without quotes in your collection to make sure your annotation process ran correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_unprocessed = collection.count_documents({'quotes' : { '$exists': False }})\n",
    "print(\"{} stories unprocessed\".format(stories_unprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Optional Adjustments for Quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine if a given quote is a scare quote or title in quotation marks. This heuristic is based on our assessment that a quote comprising three words or fewer is typically a scare quote or a title in quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scare_quote_or_title(quote):\n",
    "    quote_split = quote['text'].split()\n",
    "    if len(quote_split) < 4 or quote['text'].istitle():\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoreNLP often uses a pronoun or 'Unknown' for attribution in the `speaker` and/or `canonical_speaker` categories. This heuristic forces replaces that pronoun or 'Unknown' with a proper noun if a proper noun exists in either the `speaker` or `canonical_speaker` attribute in the extracted quote's collection entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_speakers = ['he', 'his', 'she', 'her', 'Unknown']\n",
    "\n",
    "def assumed_speaker(speaker, canonical_speaker):\n",
    "    if speaker in not_speakers and canonical_speaker in not_speakers:\n",
    "        return speaker\n",
    "    elif speaker not in not_speakers and canonical_speaker not in not_speakers:\n",
    "        return speaker\n",
    "    elif speaker in not_speakers:\n",
    "        return canonical_speaker\n",
    "    elif canonical_speaker in not_speakers:\n",
    "        return speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add snippet from `story_text` to quote entry that shows quote context. This is useful if you are trying to determine if CoreNLP's quote attribution is correct. This example code creates snippets that are a maximum of 800 characters longer than the extracted quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_snippet(story, quote):\n",
    "    snippet_begin = max(0, quote['begin_char']-400)\n",
    "    snippet_end = max(0, quote['end_char']+400)\n",
    "    new_snippet = story['story_text'][snippet_begin:snippet_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('annotated_quotes.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = ['_id','stories_id', 'text'])\n",
    "    writer.writeheader()\n",
    "    for story in collection.find({'quotes' : { '$exists': True }}):\n",
    "        for quote in story:\n",
    "            writer.writerow(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful pymongo commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count documents in collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete a key from all documents. You can also use `update_one` to run this operation on a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.collection.update_many({}, { '$unset' : { 'snippet'  : 1} })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty a collection (deletes all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
